\documentclass[11pt]{exam}
\usepackage{amsfonts}
\usepackage{physics}
\usepackage{cases}
\usepackage{algorithm}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{amsthm,amsmath,amssymb,amsfonts,bm,physics}
\usepackage{array}
\usepackage{upgreek}
\usepackage{algorithmic}
\usepackage{xcolor}
\usepackage{bbm}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage[hidelinks]{hyperref}
\newcommand*\widefbox[1]{\fbox{\hspace{2em}#1\hspace{2em}}}

\newcommand{\gr}{\selectlanguage{greek}}

%\newcommand{\red}{\color{myred}}
%\newcommand{\blue}{\color{myblue}}
%\newcommand{\black}{\color{myblack}}


%\definecolor{BrickRed}{cmyk}{0,0.89,0.94,0.28}
%\definecolor{pink}{RGB}{255,192,203}

\begin{document}
\centerline{\Large \sc Homework 10}
\pagestyle{empty}

\hrulefill

\vspace{2cm}


{\Large \sc Name:}



\vspace{2cm}



{\Large \sc Student ID:}

\vspace{6cm}

\begin{itemize}
  \item Reasoning and work must be shown to gain partial/full
  credit
  \item Please include the cover-page on your homework PDF with your name and student ID. Failure of doing so is considered bad citizenship. 

 \end{itemize}

\clearpage



\begin{questions}
\question[6]{\bf Graph Signal Processing}:
In this problem, you will use a existing package ``pygsp\footnote{\url{https://pygsp.readthedocs.io/en/stable/}}" to finish some simple tasks regarding to graph signal processing.
\begin{parts}
\part[0.5] Generate ErdosRenyi graph with $N = 50$, and $p=0.1$ and ring graph $N = 50$ by pygsp, e.g. \textit{GR = graphs.Ring(N=50)}. Plot the two graphs. 
   \part[0.5] Load the citation network dataset, Cora and plot this graph. The Cora dataset consists of 2708 scientific publications classified into one of seven classes. The citation network consists of 5429 links. Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary. The dictionary consists of 1433 unique words. (hint: \textit{from torch\_geometric.datasets import Planetoid}  and \textit{dataset = Planetoid(root='/tmp/Cora', name='Cora')}) 
\part[1 ]We consider here only undirected graphs, such that the Laplacian matrix is real symmetric, thus diagonalizable in an orthonormal eigenbasis (i.e., eigen-decomposition)
$$\mathbf{L}=\mathbf{U}\mathbf{\Lambda U}^\top,$$
where $\mathbf{U}=(\mathbf{u}_1,\ldots,\mathbf{u}_N)\in\mathbb{R}^{N\times N}$ is the matrix of orthonormal eigenvectors and $\mathbf{\Lambda} = \text{diag}(\lambda_1,\ldots,\lambda_N)$ is the diagonal matrix of associated sorted eigenvalues:
$$\lambda_1\leq\lambda_2\leq\ldots\leq\lambda_N.$$ 
Plot the graph frequencies of the ErdosRenyi, pygsp and Cora graphs from the smallest to largest.  Plot $\mathbf{u}_2$, $\mathbf{u}_3$ and $\mathbf{u}_6$ over graph, as shown in Slide 39, lecture 22 (hint: \url{https://networkx.org/documentation/stable/auto_examples/drawing/plot_node_colormap.html}).


\part[1]  The spectral content of a signal indicates if the signal is low-pass, band-pass, or high-pass. Again, intuition transfers from classical Fourier analysis.

\begin{enumerate}
	\item Build a graph by \textit{G = graphs.Sensor(N=100, seed=1)}.
	\item Compute the graph fourier basis by \textit{G.compute\_fourier\_basis()}.
	\item Create two kinds of graph signals as follows. 
	\begin{enumerate}
	\item  Generate   random signals by \textit{$\bm{x}_0$ = np.random.RandomState(10).normal(size=G.N)}. 
	\item   Define two filters \textit{g = pygsp.filters.Heat(G, tau)} by setting $tau = 0$ and $tau = 10$, respectively. 
	\item Use the two filters to process the above random signals $\bm{x}_0$ by $\bm{x} = g.filter(\bm{x}_0)$, and then obtain two kinds of graph signals, where one is low-pass and one is high-pass. 
	\end{enumerate} 
	\item Calculate the graph Fourier transform for the above low-pass and one is high-pass graph signals to obtain the graph signal in Graph Fourier domain \textit{$\hat{\bm{x}}$ = G.gft($\bm{x}$)}.
	\item Plot the graph spectral content of $\hat{\bm{x}}$, i.e. $\abs{\hat{\bm{x}}}$.
	\item Evaluate    the smoothness of the  two graph signals, i.e. $\bm{x}^\top \mathbf{L} \bm{x}$
	\item Is the graph signal generated by $tau=0$  a high-pass graph signal or low-pass graph signals?
\end{enumerate} 
\part[1] Consider the feature data of Cora. Compute the covariance matrix, and utilize the TSNE to obtain two principal components for  clustering, i.e., \textit{n\_components=2}. 

\part[2] Let's define a low-pass filter
$$\tilde{h}(\lambda) = \frac{1}{1+\alpha\lambda}$$
Given a noisy version of a smooth signal $\bm{x}_\text{noisy}$, one can denoise it with the low-pass filter $g$:
$$ x_\text{denoised} = \mathbf{U}\tilde{h}(\mathbf{\Lambda})\mathbf{U}^\top \bm{x}_{\text{noisy}}$$
\begin{enumerate}
\item Consider the ring graph	\textit{GR = graphs.Ring(N=50)}
\item Consider the following graph signals $\bm{x} = [\sin(i) + \cos(2i) + 0.5\cos(30i)]$, where $i =  np.linspace(-np.pi+2*np.pi*j/N, np.pi+2*np.pi*j/N)$, $j = 0, \cdots, N-1$.
\item Add white noise to $x$ by $np.random.uniform(-0.4, 0.4, size=N)$ to graph signals  $\bm{x}$.
\item Plot the noisy graph signals.
\item Compute the Graph Fourier domain $\hat{\bm{x}}$,  and plot the graph spectral content of $\hat{\bm{x}}$,  i.e. $\abs{\hat{\bm{x}}}$
\item Define the low-pass filter  $$\tilde{h}(\lambda) = \frac{1}{1+\alpha\lambda}$$ by setting $\alpha  = 2$ by $g = pygsp.filters.Filter(G, g)$.
\item Utilize $g$ to process the noisy graph signals $\bm{x}$ with method $x\_denoised = g.filter(x, method='exact')$.
\item Plot the denoisy graph graph signals, and its  graph spectral content.
\end{enumerate}


\end{parts}


\newpage
\question[4]{\bf Graph Neural Networks}: In this problem, you will use an existing package ``pyg\footnote{\url{https://github.com/pyg-team/pytorch_geometric}}" to build your own graph neural networks to finish some simple classification tasks. There is a tutorial of graph neural network implementation \footnote{\url{https://pytorch-geometric.readthedocs.io/en/latest/notes/introduction.html}}.
 
\begin{parts}
  \part[2]  Build a two-layer Chebyshev Graph Neural Network class by \textit{from torch\_geometric.nn import ChebConv} in Pytorch. For the first layer, the input channel is the dataset feature number, and output channel is 40, the order of GSO $K= 5$. For the second layer, the input channel is 40 and the output channel is dataset label classes.
  In the forward function, you should use the \textit{relu} as the first layer activation function, and the output activation is \textit{log\_softmax} to obtain the predicted labels.

   \part[1] Train your graph neural network by utilizing \textit{torch.optim.Adam(model.parameters(), lr=0.01, weight\_decay=5e-4)}. You can change the learning rate $lr$ and \textit{weight\_decay} according to your own model. And loss function is suggested to use \textit{torch.nn.functional.nll\_loss} or any other loss functions that are suitable for your GCN model. Plot your training loss with the epoch as the x-axis.
     \part[1] Test your graph neural network to report the accuracy. Visualize the graph with node labels for both the ground truth and the predicted ones. 
\end{parts}
    


\end{questions}





\end{document}




